{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if working in colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment if using colab\n",
    "!pip install -q -U datasets\n",
    "!pip install seqeval\n",
    "!pip install -q -U evaluate\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TextGenerationPipeline\n",
    "import torch\n",
    "import os\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict for the entities (entity to int value)\n",
    "simple_ent = {\"Condition\", \"Value\", \"Drug\", \"Procedure\", \"Measurement\", \"Temporal\", \"Observation\", \"Person\", \"Device\"}\n",
    "sel_ent = {\n",
    "    \"O\": 0,\n",
    "    \"B-Condition\": 1,\n",
    "    \"I-Condition\": 2,\n",
    "    \"B-Value\": 3,\n",
    "    \"I-Value\": 4,\n",
    "    \"B-Drug\": 5,\n",
    "    \"I-Drug\": 6,\n",
    "    \"B-Procedure\": 7,\n",
    "    \"I-Procedure\": 8,\n",
    "    \"B-Measurement\": 9,\n",
    "    \"I-Measurement\": 10,\n",
    "    \"B-Temporal\": 11,\n",
    "    \"I-Temporal\": 12,\n",
    "    \"B-Observation\": 13,\n",
    "    \"I-Observation\": 14,\n",
    "    \"B-Person\": 15,\n",
    "    \"I-Person\": 16,\n",
    "    \"B-Device\": 17,\n",
    "    \"I-Device\": 18\n",
    "}\n",
    "\n",
    "entities_list = list(sel_ent.keys())\n",
    "sel_ent_inv = {v: k for k, v in sel_ent.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '..'\n",
    "root = './drive/MyDrive/TER-LISN-2024'\n",
    "data_path = f'{root}/data'\n",
    "models_path = f'{root}/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"BioMistral/BioMistral-7B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base model(Mistral 7B)\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit= True,\n",
    "    bnb_4bit_quant_type= \"nf4\",\n",
    "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant= False,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "   model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\": 0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tokenizer for mistral-7B\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.padding_side = 'left'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.add_bos_token, tokenizer.add_eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe  = TextGenerationPipeline(model = model, tokenizer = tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('JavierLopetegui/chia_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each sentence save the text\n",
    "def generate_sentences_from_tokens(sentences):\n",
    "    texts_sentences = []\n",
    "    sentences_tokens = sentences['tokens']\n",
    "    for sentence in sentences_tokens:\n",
    "        sent_text = \" \".join(sentence)\n",
    "        texts_sentences.append(sent_text)\n",
    "    sentences['text'] = texts_sentences\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompts(sentences, prompt_type=2):\n",
    "    sentences_prompts = []\n",
    "    for sent in sentences['text']:\n",
    "        prompt = build_prompt(sent, prompt_type)\n",
    "        sentences_prompts.append(prompt)\n",
    "    sentences['prompt'] = sentences_prompts\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: generate_sentences_from_tokens(x), batched = True)\n",
    "dataset_prompt2 = dataset.map(lambda x: build_prompts(x, prompt_type=2), batched = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_p2 = dataset_prompt2['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep just the prompt column\n",
    "test_dataset_p2 = test_dataset_p2.remove_columns(['tokens', 'text', 'ner_tags', 'file'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loader_p2 = DataLoader(test_dataset_p2, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated_sentences_p2 = pipe(batch['prompt'], max_new_tokens = 500, return_full_text = False, handle_long_generation = \"hole\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_sentences_p2 = []\n",
    "for sentence in test_dataset_p2['prompt']:\n",
    "    output = pipe(sentence, max_new_tokens = 500, return_full_text = False, handle_long_generation = \"hole\")[0]['generated_text']\n",
    "    output = output.split('\\n\\n')[0]\n",
    "    generated_sentences_p2.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('xlm-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and align the labels in the dataset\n",
    "def tokenize_and_align_labels(sentence, flag = 'I'):\n",
    "    \"\"\"\n",
    "    Tokenize the sentence and align the labels\n",
    "    inputs:\n",
    "        sentence: dict, the sentence from the dataset\n",
    "        flag: str, the flag to indicate how to deal with the labels for subwords\n",
    "            - 'I': use the label of the first subword for all subwords but as intermediate (I-ENT)\n",
    "            - 'B': use the label of the first subword for all subwords as beginning (B-ENT)\n",
    "            - None: use -100 for subwords\n",
    "    outputs:\n",
    "        tokenized_sentence: dict, the tokenized sentence now with a field for the labels\n",
    "    \"\"\"\n",
    "    tokenized_sentence = tokenizer(sentence['tokens'], is_split_into_words=True, truncation=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, labels_s in enumerate(sentence['ner_tags']):\n",
    "        word_ids = tokenized_sentence.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # if the word_idx is None, assign -100\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # if it is a new word, assign the corresponding label\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(labels_s[word_idx])\n",
    "            # if it is the same word, check the flag to assign\n",
    "            else:\n",
    "                if flag == 'I':\n",
    "                    if entities_list[labels_s[word_idx]].startswith('I'):\n",
    "                      label_ids.append(labels_s[word_idx])\n",
    "                    else:\n",
    "                      label_ids.append(labels_s[word_idx] + 1)\n",
    "                elif flag == 'B':\n",
    "                    label_ids.append(labels_s[word_idx])\n",
    "                elif flag == None:\n",
    "                    label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    tokenized_sentence['labels'] = labels\n",
    "    return tokenized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standarizing true annotations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_true_annotations = []\n",
    "for sent in dataset['test']:\n",
    "    annotation = []\n",
    "    for word, tag in zip(sent['tokens'], sent['ner_tags']):\n",
    "        annotation.append((word, sel_ent[tag]))\n",
    "    new_annotation = []\n",
    "    ps = r'(\\.|\\,|\\:|\\;|\\!|\\?|\\-|\\(|\\)|\\[|\\]|\\{|\\}|\\\")'\n",
    "    for i,(word, tag) in enumerate(annotation):\n",
    "        if re.search(ps, word):\n",
    "            # find the ocurrences of the punctuation signs\n",
    "            occurrences = re.finditer(ps, word)\n",
    "            indexes = [(match.start(), match.end()) for match in occurrences]\n",
    "            # create the new tokens\n",
    "            last = 0\n",
    "            for j, (beg, end) in enumerate(indexes):\n",
    "                if beg > last:\n",
    "                    new_annotation.append((word[last:beg], tag))\n",
    "                if tag != \"O\":\n",
    "                    label = f'I-{tag.split(\"-\")[1]}'\n",
    "                else:\n",
    "                    label = \"O\"\n",
    "                if end < len(word) or (i < len(annotation) - 1 and annotation[i+1][1] == label):\n",
    "                    new_annotation.append((word[beg:end], label))\n",
    "                else:\n",
    "                    new_annotation.append((word[beg:end], 'O')) \n",
    "                last = end\n",
    "            if last < len(word):\n",
    "                new_annotation.append((word[last:], label))\n",
    "        else:\n",
    "            new_annotation.append((word, tag))\n",
    "    new_true_annotations.append(new_annotation)\n",
    "len(new_true_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_annotations = []\n",
    "for sent in new_true_annotations:\n",
    "    dicc_sent = {\"tokens\":[], \"ner_tags\":[]}\n",
    "    for word, tag in sent:\n",
    "        dicc_sent[\"tokens\"].append(word)\n",
    "        dicc_sent[\"ner_tags\"].append(sel_ent[tag])\n",
    "    true_annotations.append(dicc_sent)\n",
    "len(true_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_df = pd.DataFrame(true_annotations)\n",
    "true_ann_dataset = Dataset.from_pandas(true_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_ann_dataset = true_ann_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(p):\n",
    "    predictions, labels = p\n",
    "    # Remove ignored index (special tokens)\n",
    "    predictions = [\n",
    "        [entities_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    labels = [\n",
    "        [entities_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    return predictions, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_file import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluating prompt 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ann2bio(sentence, pattern, pattern1, pattern2):\n",
    "    if sentence[-1] == \"\\n\":\n",
    "        sentence = sentence[:-2] # remove the \\n and a final point wrongly added\n",
    "    else:\n",
    "        sentence = sentence[:-1] # remove the final point wrongly added\n",
    "    \n",
    "    # find the entities\n",
    "    occurrences = re.finditer(pattern, sentence)\n",
    "    indexes = [(match.start(), match.end()) for match in occurrences]\n",
    "\n",
    "    annotation = []\n",
    "    i = 0\n",
    "    # create the bio list\n",
    "    for beg, end in indexes:\n",
    "        if beg > i:\n",
    "            annotation.extend([(word, \"O\") for word in sentence[i:beg].split()])\n",
    "        entity = sentence[beg:end]\n",
    "        entity_name = re.search(pattern1, entity).group(1)\n",
    "        entity = entity.replace(f'<{entity_name}>', \"\").replace(f'</{entity_name}>', \"\")\n",
    "        split_entity = entity.split()\n",
    "        annotation.append((split_entity[0], \"B-\" + entity_name))\n",
    "        annotation.extend([(word, \"I-\" + entity_name) for word in split_entity[1:]])\n",
    "        i = end\n",
    "    annotation.extend([(word, \"O\") for word in sentence[i:].split()])\n",
    "\n",
    "    # check punctuation sign in tokens and put them as individual tokens\n",
    "    ps = r'(\\.|\\,|\\:|\\;|\\!|\\?|\\-|\\(|\\)|\\[|\\]|\\{|\\}|\\\")'\n",
    "    new_annotation = []\n",
    "    for i,(word, tag) in enumerate(annotation):\n",
    "        if re.search(ps, word):\n",
    "            # find the ocurrences of the punctuation signs\n",
    "            occurrences = re.finditer(ps, word)\n",
    "            indexes = [(match.start(), match.end()) for match in occurrences]\n",
    "            # create the new tokens\n",
    "            last = 0\n",
    "            for j, (beg, end) in enumerate(indexes):\n",
    "                if beg > last:\n",
    "                    new_annotation.append((word[last:beg], tag))\n",
    "                if tag != \"O\":\n",
    "                    label = f'I-{tag.split(\"-\")[1]}'\n",
    "                else:\n",
    "                    label = \"O\"\n",
    "                if end < len(word) or (i < len(annotation) - 1 and annotation[i+1][1] == label):\n",
    "                    new_annotation.append((word[beg:end], label))\n",
    "                else:\n",
    "                    new_annotation.append((word[beg:end], 'O')) \n",
    "                last = end\n",
    "            if last < len(word):\n",
    "                new_annotation.append((word[last:], label))   \n",
    "                \n",
    "        else:\n",
    "            new_annotation.append((word, tag))\n",
    "\n",
    "    \n",
    "    return new_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern1 = r'<(Person|Condition|Value|Drug|Procedure|Measurement|Temporal|Observation|Mood|Pregnancy_considerations|Device)>'\n",
    "pattern2 = r'</(Person|Condition|Value|Drug|Procedure|Measurement|Temporal|Observation|Mood|Pregnancy_considerations|Device)>'\n",
    "pattern = f'{pattern1}.*?{pattern2}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_p2_annotations = []\n",
    "for sent in generated_sentences_p2:\n",
    "    annotation = parse_ann2bio(sent, pattern, pattern1, pattern2)\n",
    "    p2_annotations.append(annotation)\n",
    "len(new_p2_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_annotations = []\n",
    "for sent in new_p2_annotations:\n",
    "    dicc_sent = {\"tokens\":[], \"ner_tags\":[]}\n",
    "    for word, tag in sent:\n",
    "        dicc_sent[\"tokens\"].append(word)\n",
    "        dicc_sent[\"ner_tags\"].append(sel_ent[tag])\n",
    "    p2_annotations.append(dicc_sent)\n",
    "len(p2_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_df = pd.DataFrame(p2_annotations)\n",
    "p2_dataset = Dataset.from_pandas(p2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_dataset = p2_dataset.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep just sentences with the same length\n",
    "sentences_to_evaluate_p2 = []\n",
    "sentences_to_evaluate_true = []\n",
    "\n",
    "for i in range(len(p2_dataset)):\n",
    "    if len(p2_dataset['labels'][i]) == len(true_ann_dataset['labels'][i]):\n",
    "        sentences_to_evaluate_p2.append(p2_dataset[labels][i])\n",
    "        sentences_to_evaluate_true.append(true_ann_dataset[labels][i])\n",
    "\n",
    "print(len(sentences_to_evaluate_p2)/len(p2_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BioEval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels, true_labels = get_labels((sentences_to_evaluate_p2, sentences_to_evaluate_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate_annotations(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.save_evaluation('eval_p2.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "print(\"done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TER",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
